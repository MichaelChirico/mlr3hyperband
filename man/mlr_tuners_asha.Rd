% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/TunerAsha.R
\name{mlr_tuners_asha}
\alias{mlr_tuners_asha}
\alias{TunerAsha}
\title{Tuner Asynchronous Successive Halving}
\description{
\code{TunerAsha} class that implements the asynchronous successive halving
algorithm. Asynchronous successive halving (ASHA) parallelizes SHA
(\link{TunerSuccessiveHalving}) by promoting candidates to the next stage as soon
as possible instead of waiting for all candidates in the stage to finish.
ASHA starts with sampling a candidate hyperparameter configuration for each
available worker. When an evaluation finishes and the worker is available
again, ASHA checks the stages from top to bottom for promotable candidates.
Promotions are possible when the evaluated candidates belong to the top
\code{1 / eta} of each stage. If no promotions are possible, a new candidate is
sampled and added to the base stage, which increases the number of possible
promotions for all stages.

The budget hyperparameter must be tagged with \code{"budget"} in the search space.
The minimum budget (\code{r_min}) which is allocated in the base stage, is set by
the lower bound of the budget parameter. The upper bound defines the maximum
budget (\code{r_max}) which which is allocated to the candidates in the last
stage.
}
\section{Parameters}{

\describe{
\item{\code{eta}}{\code{numeric(1)}\cr
With every stage, the budget is increased by a factor of \code{eta}
and only the best \code{1 / eta} candidates are promoted to the next stage.
Non-integer values are supported, but \code{eta} is not allowed to be less or
equal 1.
}
\item{\code{sampler}}{\link[paradox:Sampler]{paradox::Sampler}\cr
Object defining how the samples of the parameter space should be drawn. The
default is uniform sampling.
}}
}

\section{Archive}{

The \link[bbotk:Archive]{bbotk::Archive} holds the following additional column that is specific
to the successive halving algorithm:
\itemize{
\item \code{stage} (\verb{integer(1))}\cr
Stage index. Starts counting at 0.
}
}

\section{Custom sampler}{

All algorithms support custom \link[paradox:Sampler]{paradox::Sampler}s to draw candidates in the
base stage.\preformatted{# - beta distribution with alpha = 2 and beta = 5
# - categorical distribution with custom probabilities
sampler = SamplerJointIndep$new(list(
  Sampler1DRfun$new(params[[2]], function(n) rbeta(n, 2, 5)),
  Sampler1DCateg$new(params[[3]], prob = c(0.2, 0.3, 0.5))
))
}
}

\section{Progress Bars}{

\verb{$optimize()} supports progress bars via the package \CRANpkg{progressr}
combined with a \link{Terminator}. Simply wrap the function in
\code{progressr::with_progress()} to enable them. We recommend to use package
\CRANpkg{progress} as backend; enable with \code{progressr::handlers("progress")}.
}

\section{Parallelization}{

The hyperparameter configurations are asynchronously evaluated with the
\CRANpkg{future} package. The resampling of each candidate is send to an
available worker. To select a parallel backend, use \code{\link[future:plan]{future::plan()}}.
}

\section{Logging}{

Hyperband uses a logger (as implemented in \CRANpkg{lgr}) from package
\CRANpkg{bbotk}.
Use \code{lgr::get_logger("bbotk")} to access and control the logger.
}

\examples{
if(requireNamespace("xgboost")) {
  library(mlr3learners)

  # define hyperparameter and budget parameter
  search_space = ps(
    nrounds = p_int(lower = 1, upper = 16, tags = "budget"),
    eta = p_dbl(lower = 0, upper = 1),
    booster = p_fct(levels = c("gbtree", "gblinear", "dart"))
  )

  \donttest{
  # hyperparameter tuning on the pima indians diabetes data set
  instance = tune(
    method = "asha",
    task = tsk("pima"),
    learner = lrn("classif.xgboost", eval_metric = "logloss"),
    resampling = rsmp("cv", folds = 3),
    measures = msr("classif.ce"),
    search_space = search_space,
    term_evals = 100
  )

  # best performing hyperparameter configuration
  instance$result
  }
}
}
\section{Super classes}{
\code{\link[mlr3tuning:Tuner]{mlr3tuning::Tuner}} -> \code{\link[mlr3tuning:TunerFromOptimizer]{mlr3tuning::TunerFromOptimizer}} -> \code{TunerAsha}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-new}{\code{TunerAsha$new()}}
\item \href{#method-optimize}{\code{TunerAsha$optimize()}}
\item \href{#method-clone}{\code{TunerAsha$clone()}}
}
}
\if{html}{
\out{<details open ><summary>Inherited methods</summary>}
\itemize{
\item \out{<span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format">}\href{../../mlr3tuning/html/Tuner.html#method-format}{\code{mlr3tuning::Tuner$format()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print">}\href{../../mlr3tuning/html/Tuner.html#method-print}{\code{mlr3tuning::Tuner$print()}}\out{</span>}
}
\out{</details>}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-new"></a>}}
\if{latex}{\out{\hypertarget{method-new}{}}}
\subsection{Method \code{new()}}{
Creates a new instance of this \link[R6:R6Class]{R6} class.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TunerAsha$new()}\if{html}{\out{</div>}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-optimize"></a>}}
\if{latex}{\out{\hypertarget{method-optimize}{}}}
\subsection{Method \code{optimize()}}{
Performs the tuning on a \link{TuningInstanceSingleCrit} /
\link{TuningInstanceMultiCrit} until termination. The single evaluations and
the final results will be written into the \link{ArchiveTuning} that
resides in the \link{TuningInstanceSingleCrit}/\link{TuningInstanceMultiCrit}.
The final result is returned.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TunerAsha$optimize(inst)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{inst}}{(\link{TuningInstanceSingleCrit} | \link{TuningInstanceMultiCrit}).}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
\link[data.table:data.table]{data.table::data.table}.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-clone"></a>}}
\if{latex}{\out{\hypertarget{method-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TunerAsha$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
