% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/TunerAhb.R
\name{mlr_tuners_ahb}
\alias{mlr_tuners_ahb}
\alias{TunerAhb}
\title{Tuner Asynchronous Hyperband}
\description{
\code{TunerAhb} class that implements the asynchronous hyperband algorithm (AHB).
AHB repeatedly runs ASHA (\link{TunerAsha}) with different minimum budgets in the
base stage. Each run of ASHA within AHB is called a bracket. AHB considers
\code{s_max + 1} brackets with \verb{s_max = floor(log(r_max / r_min, eta)}. The most
explorative bracket \code{s = s_max} constructs \code{s_max + 1} stages and allocates
the minimum budget (\code{r_min}) in the base stage.  The minimum budget (\code{r_min})
is increased in each bracket by a factor of \code{eta} until the maximum budget is
allocated in the base stage. The bracket \code{s = 0} is a random search with full
budget. Each ASHA run uses \code{1 / s_max + 1} of the \link[bbotk:Terminator]{bbotk::Terminator}.

The budget hyperparameter must be tagged with \code{"budget"} in the search space.
The minimum budget (\code{r_min}) which is allocated in the base stage of the most
explorative bracket, is set by the lower bound of the budget parameter. The
upper bound defines the maximum budget (\code{r_max}) which which is allocated to
the candidates in the last stages.
}
\section{Parameters}{

\describe{
\item{\code{eta}}{\code{numeric(1)}\cr
With every stage, the budget is increased by a factor of \code{eta}
and only the best \code{1 / eta} candidates are promoted to the next stage.
Non-integer values are supported, but \code{eta} is not allowed to be less or
equal 1.
}
\item{\code{sampler}}{\link[paradox:Sampler]{paradox::Sampler}\cr
Object defining how the samples of the parameter space should be drawn in the
base stage of each bracket. The default is uniform sampling.
}
\item{\code{adjust_minimum_budget}}{\code{logical(1)}\cr
If \code{TRUE}, minimum budget is increased so that the last stage uses the
maximum budget defined in the search space.
}}
}

\section{Archive}{

The \link[mlr3tuning:ArchiveTuning]{mlr3tuning::ArchiveTuning} holds the following additional columns that
are specific to the hyperband algorithm:
\itemize{
\item \code{bracket} (\code{integer(1)})\cr
The bracket index. Counts down to 0.
\item \code{stage} (\verb{integer(1))}\cr
The stages of each bracket. Starts counting at 0.
}
}

\section{Custom Sampler}{


\CRANpkg{mlr3hyperband} supports custom \link[paradox:Sampler]{paradox::Sampler} to draw candidates
in the base stage.\preformatted{# - beta distribution with alpha = 2 and beta = 5
# - categorical distribution with custom probabilities
sampler = SamplerJointIndep$new(list(
  Sampler1DRfun$new(params[[2]], function(n) rbeta(n, 2, 5)),
  Sampler1DCateg$new(params[[3]], prob = c(0.2, 0.3, 0.5))
))
}
}

\section{Progress Bars}{


The  \verb{$optimize()} method supports progress bars via the package
\CRANpkg{progressr}. Simply wrap the method call in \code{progressr::with_progress()}
to enable them.
Alternatively, call \code{\link[progressr:handlers]{progressr::handlers()}} with \code{global = TRUE} to enable progress bars
globally.
We recommend the \CRANpkg{progress} package as backend which can be enabled with
\code{progressr::handlers("progress")}.
}

\section{Parallelization}{

The hyperparameter configurations are asynchronously evaluated with
the \CRANpkg{future} package. The resampling of each candidate is send to an
available worker. To select a parallel backend, use \code{\link[future:plan]{future::plan()}}.
}

\section{Logging}{


\CRANpkg{mlr3hyperband} uses the \CRANpkg{lgr} package for logging.
\CRANpkg{lgr} supports multiple log levels which can be queried with
\code{getOption("lgr.log_levels")}. Use \code{lgr::get_logger("bbotk")} to access and
control the logger.

To suppress output and reduce verbosity, you can lower the log from the
default level \code{"info"} to \code{"warn"}:\preformatted{lgr::get_logger("bbotk")$set_threshold("warn")
}

To log to a file or a data base, see the documentation of \link[lgr:lgr-package]{lgr::lgr-package}.
}

\examples{
if(requireNamespace("xgboost")) {
  library(mlr3learners)

  # define hyperparameter and budget parameter
  search_space = ps(
    nrounds = p_int(lower = 1, upper = 16, tags = "budget"),
    eta = p_dbl(lower = 0, upper = 1),
    booster = p_fct(levels = c("gbtree", "gblinear", "dart"))
  )

  \donttest{
  # hyperparameter tuning on the pima indians diabetes data set
  instance = tune(
    method = "ahb",
    task = tsk("pima"),
    learner = lrn("classif.xgboost", eval_metric = "logloss"),
    resampling = rsmp("cv", folds = 3),
    measures = msr("classif.ce"),
    search_space = search_space,
    term_evals = 100
  )

  # best performing hyperparameter configuration
  instance$result
  }
}
}
\section{Super classes}{
\code{\link[mlr3tuning:Tuner]{mlr3tuning::Tuner}} -> \code{\link[mlr3tuning:TunerFromOptimizer]{mlr3tuning::TunerFromOptimizer}} -> \code{TunerAhb}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-new}{\code{TunerAhb$new()}}
\item \href{#method-optimize}{\code{TunerAhb$optimize()}}
\item \href{#method-clone}{\code{TunerAhb$clone()}}
}
}
\if{html}{
\out{<details open ><summary>Inherited methods</summary>}
\itemize{
\item \out{<span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format">}\href{../../mlr3tuning/html/Tuner.html#method-format}{\code{mlr3tuning::Tuner$format()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print">}\href{../../mlr3tuning/html/Tuner.html#method-print}{\code{mlr3tuning::Tuner$print()}}\out{</span>}
}
\out{</details>}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-new"></a>}}
\if{latex}{\out{\hypertarget{method-new}{}}}
\subsection{Method \code{new()}}{
Creates a new instance of this \link[R6:R6Class]{R6} class.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TunerAhb$new()}\if{html}{\out{</div>}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-optimize"></a>}}
\if{latex}{\out{\hypertarget{method-optimize}{}}}
\subsection{Method \code{optimize()}}{
Performs the tuning on a \link{TuningInstanceSingleCrit} /
\link{TuningInstanceMultiCrit} until termination. The single evaluations and
the final results will be written into the \link{ArchiveTuning} that
resides in the \link{TuningInstanceSingleCrit}/\link{TuningInstanceMultiCrit}.
The final result is returned.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TunerAhb$optimize(inst)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{inst}}{(\link{TuningInstanceSingleCrit} | \link{TuningInstanceMultiCrit}).}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
\link[data.table:data.table]{data.table::data.table}.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-clone"></a>}}
\if{latex}{\out{\hypertarget{method-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TunerAhb$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
